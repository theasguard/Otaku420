import itertools
import json
import pickle
import re
import six
import urllib.parse

from functools import partial
from bs4 import BeautifulSoup, SoupStrainer
from resources.lib import debrid
from resources.lib.ui import control, database, source_utils
from resources.lib.ui.BrowserBase import BrowserBase
from six.moves import urllib_parse


class sources(BrowserBase):
    _BASE_URL = 'https://nyaa-si.translate.goog/?_x_tr_sl=es&_x_tr_tl=en&_x_tr_hl=en/' if control.getSetting('provider.nyaaalt') == 'true' else 'https://nyaa.si/'

    @staticmethod
    def _parse_nyaa_episode_view(res, episode):
        source = {
            'release_title': res['name'] if six.PY2 else res['name'],
            'hash': res['hash'],
            'type': 'torrent',
            'quality': source_utils.getQuality(res['name']),
            'debrid_provider': res['debrid_provider'],
            'provider': 'nyaa',
            'episode_re': episode,
            'size': res['size'],
            'info': source_utils.getInfo(res['name']),
            'lang': source_utils.getAudio_lang(res['name'])
        }
    
        return source
    
    @staticmethod
    def _parse_nyaa_cached_episode_view(res, episode):
        source = {
            'release_title': res['name'] if six.PY2 else res['name'],
            'hash': res['hash'],
            'type': 'torrent',
            'quality': source_utils.getQuality(res['name']),
            'debrid_provider': res['debrid_provider'],
            'provider': 'nyaa (Local Cache)',
            'episode_re': episode,
            'size': res['size'],
            'info': source_utils.getInfo(res['name']),
            'lang': source_utils.getAudio_lang(res['name'])
        }
    
        return source

    def _process_nyaa_episodes(self, url, episode, season=None):
        html = self._get_request(url)
        mlink = SoupStrainer('div', {'class': 'table-responsive'})
        soup = BeautifulSoup(html, "html.parser", parse_only=mlink)
        rex = r'(magnet:)+[^"]*'
    
        list_ = [
            {'magnet': i.find('a', {'href': re.compile(rex)}).get('href'),
             'name': i.find_all('a', {'class': None})[1].get('title'),
             'size': i.find_all('td', {'class': 'text-center'})[1].text.replace('i', ''),
             'downloads': int(i.find_all('td', {'class': 'text-center'})[-1].text)}
            for i in soup.select("tr.danger,tr.default,tr.success")
        ]
    
        regex = r'\b(?:s|season|series)\s*(\d+)\b(?:episode|ep|eps)\s*(\d+)'
        regex_ep = r'\b(?:e|ep|eps|episode)\s*(\d{1,4})\b'
        rex = re.compile(regex)
        rex_ep = re.compile(regex_ep)
        filtered_list = []
    
        for idx, torrent in enumerate(list_):
            torrent['hash'] = re.findall(r'btih:(.*?)(?:&|$)', torrent['magnet'])[0]
            if season:
                title = torrent['name'].lower()
    
                ep_match = rex_ep.findall(title)
                ep_match = list(map(int, list(filter(None, itertools.chain(*ep_match)))))
                if ep_match and ep_match[0] != int(episode):
                    regex_ep_range = r'\s(?:\d+(?:[-~]\d+)?)(?:-(?:\d+(?:[-~]\d+)?))?'
                    rex_ep_range = re.compile(regex_ep_range)
                    if not rex_ep_range.search(title):
                        continue
    
                match = rex.findall(title)
                match = list(map(int, list(filter(None, itertools.chain(*match)))))
                if not match or match[0] == int(season):
                    filtered_list.append(torrent)
            else:
                filtered_list.append(torrent)
    
        cache_list = debrid.TorrentCacheCheck().torrentCacheCheck(filtered_list)
        cache_list = sorted(cache_list, key=lambda k: k['downloads'], reverse=True)
        mapfunc = partial(self._parse_nyaa_episode_view, episode=episode)
        all_results = list(map(mapfunc, cache_list))
        return all_results
    
    def _process_nyaa_backup(self, url, anilist_id, _zfill, episode='', rescrape=False):
        json_resp = self._get_request(url)
        results = BeautifulSoup(json_resp, 'html.parser')
        rex = r'(magnet:)+[^"]*'
        search_results = [
            (i.find_all('a', {'href': re.compile(rex)})[0].get('href'),
             i.find_all('a', {'class': None})[1].get('title'),
             i.find_all('td', {'class': 'text-center'})[1].text,
             i.find_all('td', {'class': 'text-center'})[-1].text)
            for i in results.select("tr.danger,tr.default,tr.success")
        ][:30]
        list_ = [
            {'magnet': magnet,
             'name': name,
             'size': size.replace('i', ''),
             'downloads': int(downloads)}
            for magnet, name, size, downloads in search_results
        ]
    
        for torrent in list_:
            torrent['hash'] = re.findall(r'btih:(.*?)(?:&|$)', torrent['magnet'])[0]
    
        if not rescrape:
            database.addTorrentList(anilist_id, list_, _zfill)
    
        cache_list = debrid.TorrentCacheCheck().torrentCacheCheck(list_)
        cache_list = sorted(cache_list, key=lambda k: k['downloads'], reverse=True)
        mapfunc = partial(self._parse_nyaa_episode_view, episode=episode)
        all_results = list(map(mapfunc, cache_list))
        return all_results
    
    def _process_nyaa_movie(self, url, episode):
        json_resp = self._get_request(url)
        results = BeautifulSoup(json_resp, 'html.parser')
        rex = r'(magnet:)+[^"]*'
        search_results = [
            (i.find_all('a', {'href': re.compile(rex)})[0].get('href'),
             i.find_all('a', {'class': None})[1].get('title'),
             i.find_all('td', {'class': 'text-center'})[1].text,
             i.find_all('td', {'class': 'text-center'})[-1].text)
            for i in results.select("tr.danger,tr.default,tr.success")
        ]
        list_ = [
            {'magnet': magnet,
             'name': name,
             'size': size.replace('i', ''),
             'downloads': int(downloads)}
            for magnet, name, size, downloads in search_results
        ]
    
        for idx, torrent in enumerate(list_):
            torrent['hash'] = re.findall(r'btih:(.*?)(?:&|$)', torrent['magnet'])[0]
    
        cache_list = debrid.TorrentCacheCheck().torrentCacheCheck(list_)
        cache_list = sorted(cache_list, key=lambda k: k['downloads'], reverse=True)
        mapfunc = partial(self._parse_nyaa_episode_view, episode=episode)
        all_results = list(map(mapfunc, cache_list))
        return all_results
    
    def _process_cached_sources(self, list_, episode):
        cache_list = debrid.TorrentCacheCheck().torrentCacheCheck(list_)
        mapfunc = partial(self._parse_nyaa_cached_episode_view, episode=episode)
        all_results = list(map(mapfunc, cache_list))
        return all_results

    def _get_episode_sources(self, show, anilist_id, episode, status, rescrape):
        if rescrape:
            return self._get_episode_sources_pack(show, anilist_id, episode)
        
        try:
            cached_sources, zfill_int = database.getTorrentList(anilist_id)
            if cached_sources:
                return self._process_cached_sources(cached_sources, episode.zfill(zfill_int))
        except ValueError:
            pass
        
        # Construct the base query with the show and episode information
        query = f'{show} "- {episode.zfill(2)}"'
        
        # Retrieve season information
        season = database.get_season_list(anilist_id)
        if season:
            season = str(season['season']).zfill(2)
            query += f'|"S{season}E{episode.zfill(2)}"'
        
        # Construct the URL with the base query
        url = f'{self._BASE_URL}?f=0&c=1_0&q={urllib.parse.quote_plus(query)}&s=downloads&o=desc'
        
        # Check if the status is 'FINISHED'
        if status == 'FINISHED':
            # Construct additional queries for 'Batch' and 'Complete Series' if available
            query = f'{show} "Batch"|"Complete Series"'
            episodes = pickle.loads(database.get_show(anilist_id)['kodi_meta'])['episodes']
            if episodes:
                query += f'|"01-{episodes}"|"01~{episodes}"|"01 - {episodes}"|"01 ~ {episodes}"'
        
            # Append season information if available
            if season:
                query += f'|"S{season}"|"Season {season}"'
                query += f'|"S{season}E{episode.zfill(2)}"'
        
            # Append episode information
            query += f'|"- {episode.zfill(2)}"'
            # Construct the URL with the updated query for 'FINISHED' status
            url = f'{self._BASE_URL}?f=0&c=1_0&q={urllib.parse.quote_plus(query)}&s=seeders&o=desc'
        
        # Process the nyaa.si episodes using the constructed URL
        return self._process_nyaa_episodes(url, episode.zfill(2), season)

    def _get_episode_sources_backup(self, db_query, anilist_id, episode):
        # Retrieve show information
        show = self._get_request(f'https://kaito-title.firebaseio.com/{anilist_id}.json')
        show = json.loads(show)
    
        # If show information is not available, return empty list
        if not show:
            return []
    
        # If general title is available in show information
        if 'general_title' in show:
            # Encode general title if using Python 2
            query = show['general_title'].encode('utf-8') if six.PY2 else show['general_title']
            
            # Get zfill value from show information or default to 2
            _zfill = show.get('zfill', 2)
            
            # Zero-pad the episode number based on zfill value
            episode = episode.zfill(_zfill)
            
            # Encode the query for URL
            query = urllib.parse.quote_plus(query)
            
            # Construct the URL with the encoded query
            url = f'{self._BASE_URL}?f=0&c=1_0&q={query}&s=downloads&o=desc'
            
            # Process nyaa.si backup sources
            return self._process_nyaa_backup(url, anilist_id, _zfill, episode)
    
        try:
            # Update Kodi metadata with the general title if available
            kodi_meta = pickle.loads(database.get_show(anilist_id)['kodi_meta'])
            kodi_meta['query'] = f'{db_query}|{show["general_title"]}'
            database.update_kodi_meta(anilist_id, kodi_meta)
        except Exception as e:
            pass
    
        # Construct the query with show and episode information
        query = f'{show} "- {episode.zfill(2)}"'
        
        # Retrieve season information
        season = database.get_season_list(anilist_id)
        if season:
            # Zero-pad season number
            season = str(season['season']).zfill(2)
            query += f'|"S{season}E{episode.zfill(2)}"'
    
        # Construct the URL with the query
        url = f'{self._BASE_URL}?f=0&c=1_0&q={urllib.parse.quote_plus(query)}'
        
        # Process nyaa.si episodes
        return self._process_nyaa_episodes(url, episode)

    def _get_episode_sources_pack(self, show, anilist_id, episode):
        # Construct the base query with "Batch" and "Complete Series"
        query = f'{show} "Batch"|"Complete Series"'
    
        # Retrieve episode information from Kodi metadata
        episodes = pickle.loads(database.get_show(anilist_id)['kodi_meta'])['episodes']
        if episodes:
            # Append queries for individual episodes if available
            query += f'|"01-{episodes}"|"01~{episodes}"|"01 - {episodes}"|"01 ~ {episodes}"'
    
        # Retrieve season information
        season = database.get_season_list(anilist_id)
        if season:
            # Append queries for seasons if available
            season = season['season']
            query += f'|"S{season}"|"Season {season}"'
    
        # Construct the URL with the base query
        url = f'{self._BASE_URL}?f=0&c=1_2&q={urllib.parse.quote_plus(query)}&s=seeders&o=desc'
    
        # Process the nyaa.si backup with the constructed URL
        return self._process_nyaa_backup(url, anilist_id, 2, episode.zfill(2), True)

    def _get_movie_sources(self, query, anilist_id, episode):
        # Encode the query
        query = urllib.parse.quote_plus(query)
        
        # Construct the URL with the encoded query
        url = f'{self._BASE_URL}?f=0&c=1_2&q={query}&s=downloads&o=desc'
        
        # Process nyaa.si movie sources
        sources = self._process_nyaa_movie(url, '1')
    
        # If no sources found, try getting from backup
        if not sources:
            sources = self._get_movie_sources_backup(anilist_id)
    
        return sources
    
    def _get_movie_sources_backup(self, anilist_id, episode='1'):
        # Retrieve show information
        show = self._get_request(f"https://kimetsu-title.firebaseio.com/{anilist_id}.json")
        show = json.loads(show)
        
        # If show information is not available, return empty list
        if not show:
            return []
    
        # Extract query from show information
        if 'general_title' in show:
            query = urllib.parse.quote_plus(show['general_title'])
        else:
            query = urllib.parse.quote_plus(show)
    
        # Construct the URL with the encoded query
        url = f'{self._BASE_URL}?f=0&c=1_2&q={query}&s=downloads&o=desc'
        
        # Process nyaa.si backup movie sources
        return self._process_nyaa_backup(url, episode)
